{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyHRnetSeg.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Q4FadJmqC5SQ",
        "rzqp1xWmDSdU",
        "mqjupOXqCyv0",
        "C1gFDhm7C-aG",
        "yL5O5J2_CDGF",
        "fUujZ9bZDLEK"
      ],
      "mount_file_id": "1xrA0eSU5H8EixEdD5m_WnNHkOoCqeFF1",
      "authorship_tag": "ABX9TyMS7+lfjCerqBvRcHf4W37K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shoiTK/deep_learning_with_pytorch/blob/main/MyHRnetSeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inu6ZLsXIJ_T"
      },
      "source": [
        "#install dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxPdzzwTCUax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c2f6fc-904f-4fa8-9204-9378f11de8ac"
      },
      "source": [
        "!git clone https://github.com/HRNet/HRNet-Semantic-Segmentation "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'HRNet-Semantic-Segmentation' already exists and is not an empty directory.\n",
            "fatal: destination path 'HRNet-Semantic-Segmentation' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QceQCM7iG8cQ"
      },
      "source": [
        "!pip install -r /content/HRNet-Semantic-Segmentation/requirements.txt\n",
        "!pip install imgaug==0.2.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8Bd_IRiED7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73edd427-a742-4ac5-f2b6-d602ed43b04d"
      },
      "source": [
        "!mkdir /content/HRNet-Semantic-Segmentation/data/cityscapes\n",
        "!unzip /content/drive/MyDrive/cityscapes/gtFine_trainvaltest.zip  -d /content/HRNet-Semantic-Segmentation/data/cityscapes"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/HRNet-Semantic-Segmentation/data/cityscapes’: File exists\n",
            "Archive:  /content/drive/MyDrive/cityscapes/gtFine_trainvaltest.zip\n",
            "replace /content/HRNet-Semantic-Segmentation/data/cityscapes/README? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUFecBu5oSzR",
        "outputId": "29573db2-a5b4-43d4-eef9-b8e8c42c6da6"
      },
      "source": [
        "!unzip /content/drive/MyDrive/cityscapes/leftImg8bit_trainvaltest.zip -d /content/HRNet-Semantic-Segmentation/data/cityscapes"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/cityscapes/leftImg8bit_trainvaltest.zip\n",
            "replace /content/HRNet-Semantic-Segmentation/data/cityscapes/README? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8L6pMLsQhbM",
        "outputId": "63352c30-0197-4a66-f8ed-2848e40dd44d"
      },
      "source": [
        "%cd HRNet-Semantic-Segmentation/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/HRNet-Semantic-Segmentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4FadJmqC5SQ"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zzzG79wbKWe"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import functools\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import pprint\n",
        "import shutil\n",
        "import sys\n",
        "import logging\n",
        "import time\n",
        "import timeit\n",
        "from pathlib import Path\n",
        "from tensorboardX import SummaryWriter\n",
        "import numpy.ma as ma\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch._utils\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzqp1xWmDSdU"
      },
      "source": [
        "#Second Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zuGbPMB2n15",
        "outputId": "9356c760-332e-4079-d11d-8a5469f07d79"
      },
      "source": [
        "%cd HRNet-Semantic-Segmentation/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/HRNet-Semantic-Segmentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40-zLGYriX2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99b7faa-b09c-4475-d5bf-b6d90eb788a2"
      },
      "source": [
        "import tools._init_paths\n",
        "\n",
        "%cd lib\n",
        "from utils.utils import AverageMeter\n",
        "from utils.utils import get_confusion_matrix\n",
        "from utils.utils import adjust_learning_rate\n",
        "from utils.utils import create_logger\n",
        "\n",
        "from config import update_config\n",
        "from core.criterion import CrossEntropy, OhemCrossEntropy\n",
        "%cd .."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/HRNet-Semantic-Segmentation/lib\n",
            "/content/HRNet-Semantic-Segmentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqjupOXqCyv0"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhN5tiJ3bmHT"
      },
      "source": [
        "BatchNorm2d = nn.BatchNorm2d\n",
        "BN_MOMENTUM = 0.01\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class FullModel(nn.Module):\n",
        "  def __init__(self, model, loss):\n",
        "    super(FullModel, self).__init__()\n",
        "    self.model = model\n",
        "    self.loss = loss\n",
        "\n",
        "  def forward(self, inputs, labels):\n",
        "    outputs = self.model(inputs)\n",
        "    loss = self.loss(outputs, labels)\n",
        "    return torch.unsqueeze(loss,0), outputs\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "    self.relu = nn.ReLU(inplace=False)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "  \n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "        residual = self.downsample(x)\n",
        "\n",
        "    out = out + residual\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxf7tlzcfQpB"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "    super(Bottleneck, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "    self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "    self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "    self.bn3 = BatchNorm2d(planes * self.expansion, momentum=BN_MOMENTUM)\n",
        "    self.relu = nn.ReLU(inplace=False)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "  \n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.bn3(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "        residual = self.downsample(x)\n",
        "\n",
        "    out = out + residual\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5J9U4_if32h"
      },
      "source": [
        "class HighResolutionModule(nn.Module):\n",
        "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n",
        "                 num_channels, fuse_method, multi_scale_output=True):\n",
        "      super(HighResolutionModule, self).__init__()\n",
        "      self._check_branches(num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
        "\n",
        "      self.num_inchannels = num_inchannels\n",
        "      self.fuse_method = fuse_method\n",
        "      self.num_branches = num_branches\n",
        "\n",
        "      self.multi_scale_output = multi_scale_output\n",
        "\n",
        "      self.branches = self._make_branches(num_branches, blocks, num_blocks, num_channels)\n",
        "      self.fuse_layers = self._make_fuse_layers()\n",
        "      self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def _check_branches(self, num_branches, blocks, num_blocks, num_inchannels, num_channels):\n",
        "      if num_branches != len(num_blocks):\n",
        "          error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(num_branches, len(num_blocks))\n",
        "          logger.error(error_msg)\n",
        "          raise ValueError(error_msg)\n",
        "\n",
        "      if num_branches != len(num_channels):\n",
        "          error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(num_branches, len(num_channels))\n",
        "          logger.error(error_msg)\n",
        "          raise ValueError(error_msg)\n",
        "\n",
        "      if num_branches != len(num_inchannels):\n",
        "          error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(num_branches, len(num_inchannels))\n",
        "          logger.error(error_msg)\n",
        "          raise ValueError(error_msg)\n",
        "\n",
        "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):\n",
        "      downsample = None\n",
        "      if stride != 1 or \\\n",
        "          self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
        "          downsample = nn.Sequential(\n",
        "              nn.Conv2d(self.num_inchannels[branch_index],\n",
        "                        num_channels[branch_index] * block.expansion,\n",
        "                        kernel_size=1, stride=stride, bias=False),\n",
        "              BatchNorm2d(num_channels[branch_index] * block.expansion,\n",
        "                          momentum=BN_MOMENTUM),\n",
        "          )\n",
        "\n",
        "      layers = []\n",
        "      layers.append(block(self.num_inchannels[branch_index],\n",
        "                          num_channels[branch_index], stride, downsample))\n",
        "      self.num_inchannels[branch_index] = \\\n",
        "          num_channels[branch_index] * block.expansion\n",
        "      for i in range(1, num_blocks[branch_index]):\n",
        "          layers.append(block(self.num_inchannels[branch_index],\n",
        "                              num_channels[branch_index]))\n",
        "\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
        "      branches = []\n",
        "\n",
        "      for i in range(num_branches):\n",
        "          branches.append(\n",
        "              self._make_one_branch(i, block, num_blocks, num_channels))\n",
        "\n",
        "      return nn.ModuleList(branches)\n",
        "\n",
        "    def _make_fuse_layers(self):\n",
        "      if self.num_branches == 1:\n",
        "          return None\n",
        "\n",
        "      num_branches = self.num_branches\n",
        "      num_inchannels = self.num_inchannels\n",
        "      fuse_layers = []\n",
        "      for i in range(num_branches if self.multi_scale_output else 1):\n",
        "          fuse_layer = []\n",
        "          for j in range(num_branches):\n",
        "              if j > i:\n",
        "                  fuse_layer.append(nn.Sequential(\n",
        "                      nn.Conv2d(num_inchannels[j],\n",
        "                                num_inchannels[i],\n",
        "                                1,\n",
        "                                1,\n",
        "                                0,\n",
        "                                bias=False),\n",
        "                      BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n",
        "              elif j == i:\n",
        "                  fuse_layer.append(None)\n",
        "              else:\n",
        "                  conv3x3s = []\n",
        "                  for k in range(i-j):\n",
        "                      if k == i - j - 1:\n",
        "                          num_outchannels_conv3x3 = num_inchannels[i]\n",
        "                          conv3x3s.append(nn.Sequential(\n",
        "                              nn.Conv2d(num_inchannels[j],\n",
        "                                        num_outchannels_conv3x3,\n",
        "                                        3, 2, 1, bias=False),\n",
        "                              BatchNorm2d(num_outchannels_conv3x3, \n",
        "                                          momentum=BN_MOMENTUM)))\n",
        "                      else:\n",
        "                          num_outchannels_conv3x3 = num_inchannels[j]\n",
        "                          conv3x3s.append(nn.Sequential(\n",
        "                              nn.Conv2d(num_inchannels[j],\n",
        "                                        num_outchannels_conv3x3,\n",
        "                                        3, 2, 1, bias=False),\n",
        "                              BatchNorm2d(num_outchannels_conv3x3,\n",
        "                                          momentum=BN_MOMENTUM),\n",
        "                              nn.ReLU(inplace=False)))\n",
        "                  fuse_layer.append(nn.Sequential(*conv3x3s))\n",
        "          fuse_layers.append(nn.ModuleList(fuse_layer))\n",
        "\n",
        "      return nn.ModuleList(fuse_layers)\n",
        "\n",
        "    def get_num_inchannels(self):\n",
        "      return self.num_inchannels\n",
        "\n",
        "    def forward(self, x):\n",
        "      if self.num_branches == 1:\n",
        "          return [self.branches[0](x[0])]\n",
        "\n",
        "      for i in range(self.num_branches):\n",
        "          x[i] = self.branches[i](x[i])\n",
        "\n",
        "      x_fuse = []\n",
        "      for i in range(len(self.fuse_layers)):\n",
        "          y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
        "          for j in range(1, self.num_branches):\n",
        "              if i == j:\n",
        "                  y = y + x[j]\n",
        "              elif j > i:\n",
        "                  width_output = x[i].shape[-1]\n",
        "                  height_output = x[i].shape[-2]\n",
        "                  y = y + F.interpolate(\n",
        "                      self.fuse_layers[i][j](x[j]),\n",
        "                      size=[height_output, width_output],\n",
        "                      mode='bilinear')\n",
        "              else:\n",
        "                  y = y + self.fuse_layers[i][j](x[j])\n",
        "          x_fuse.append(self.relu(y))\n",
        "\n",
        "      return x_fuse"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7yi19fNgnxZ"
      },
      "source": [
        "blocks_dict = {\n",
        "    'BASIC': BasicBlock,\n",
        "    'BOTTLENECK': Bottleneck\n",
        "}\n",
        "\n",
        "class HighResolutionNet(nn.Module):\n",
        "\n",
        "    def __init__(self, config, **kwargs):\n",
        "        extra = config['MODEL']['EXTRA']\n",
        "        super(HighResolutionNet, self).__init__()\n",
        "\n",
        "        # stem net\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "        self.stage1_cfg = extra['STAGE1']\n",
        "        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n",
        "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
        "        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n",
        "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
        "        stage1_out_channel = block.expansion*num_channels\n",
        "\n",
        "        self.stage2_cfg = extra['STAGE2']\n",
        "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition1 = self._make_transition_layer(\n",
        "            [stage1_out_channel], num_channels)\n",
        "        self.stage2, pre_stage_channels = self._make_stage(\n",
        "            self.stage2_cfg, num_channels)\n",
        "\n",
        "        self.stage3_cfg = extra['STAGE3']\n",
        "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition2 = self._make_transition_layer(\n",
        "            pre_stage_channels, num_channels)\n",
        "        self.stage3, pre_stage_channels = self._make_stage(\n",
        "            self.stage3_cfg, num_channels)\n",
        "\n",
        "        self.stage4_cfg = extra['STAGE4']\n",
        "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition3 = self._make_transition_layer(\n",
        "            pre_stage_channels, num_channels)\n",
        "        self.stage4, pre_stage_channels = self._make_stage(\n",
        "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
        "        \n",
        "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
        "\n",
        "        self.last_layer = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=last_inp_channels,\n",
        "                out_channels=last_inp_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0),\n",
        "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(\n",
        "                in_channels=last_inp_channels,\n",
        "                out_channels=config['DATASET']['NUM_CLASSES'],\n",
        "                kernel_size=extra['FINAL_CONV_KERNEL'],\n",
        "                stride=1,\n",
        "                padding=1 if extra['FINAL_CONV_KERNEL'] == 3 else 0)\n",
        "        )\n",
        "\n",
        "    def _make_transition_layer(\n",
        "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
        "        num_branches_cur = len(num_channels_cur_layer)\n",
        "        num_branches_pre = len(num_channels_pre_layer)\n",
        "\n",
        "        transition_layers = []\n",
        "        for i in range(num_branches_cur):\n",
        "            if i < num_branches_pre:\n",
        "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
        "                    transition_layers.append(nn.Sequential(\n",
        "                        nn.Conv2d(num_channels_pre_layer[i],\n",
        "                                  num_channels_cur_layer[i],\n",
        "                                  3,\n",
        "                                  1,\n",
        "                                  1,\n",
        "                                  bias=False),\n",
        "                        BatchNorm2d(\n",
        "                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n",
        "                        nn.ReLU(inplace=False)))\n",
        "                else:\n",
        "                    transition_layers.append(None)\n",
        "            else:\n",
        "                conv3x3s = []\n",
        "                for j in range(i+1-num_branches_pre):\n",
        "                    inchannels = num_channels_pre_layer[-1]\n",
        "                    outchannels = num_channels_cur_layer[i] \\\n",
        "                        if j == i-num_branches_pre else inchannels\n",
        "                    conv3x3s.append(nn.Sequential(\n",
        "                        nn.Conv2d(\n",
        "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
        "                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
        "                        nn.ReLU(inplace=False)))\n",
        "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
        "\n",
        "        return nn.ModuleList(transition_layers)\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_stage(self, layer_config, num_inchannels,\n",
        "                    multi_scale_output=True):\n",
        "        num_modules = layer_config['NUM_MODULES']\n",
        "        num_branches = layer_config['NUM_BRANCHES']\n",
        "        num_blocks = layer_config['NUM_BLOCKS']\n",
        "        num_channels = layer_config['NUM_CHANNELS']\n",
        "        block = blocks_dict[layer_config['BLOCK']]\n",
        "        fuse_method = layer_config['FUSE_METHOD']\n",
        "\n",
        "        modules = []\n",
        "        for i in range(num_modules):\n",
        "            # multi_scale_output is only used last module\n",
        "            if not multi_scale_output and i == num_modules - 1:\n",
        "                reset_multi_scale_output = False\n",
        "            else:\n",
        "                reset_multi_scale_output = True\n",
        "            modules.append(\n",
        "                HighResolutionModule(num_branches,\n",
        "                                      block,\n",
        "                                      num_blocks,\n",
        "                                      num_inchannels,\n",
        "                                      num_channels,\n",
        "                                      fuse_method,\n",
        "                                      reset_multi_scale_output)\n",
        "            )\n",
        "            num_inchannels = modules[-1].get_num_inchannels()\n",
        "\n",
        "        return nn.Sequential(*modules), num_inchannels\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        #print(x.shape)\n",
        "        x = self.layer1(x)\n",
        "        #print(x.shape)\n",
        "        x_list = []\n",
        "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
        "            if self.transition1[i] is not None:\n",
        "                x_list.append(self.transition1[i](x))\n",
        "            else:\n",
        "                x_list.append(x)\n",
        "        y_list = self.stage2(x_list)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
        "            if self.transition2[i] is not None:\n",
        "                if i < self.stage2_cfg['NUM_BRANCHES']:\n",
        "                    x_list.append(self.transition2[i](y_list[i]))\n",
        "                else:\n",
        "                    x_list.append(self.transition2[i](y_list[-1]))\n",
        "            else:\n",
        "                x_list.append(y_list[i])\n",
        "        y_list = self.stage3(x_list)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
        "            if self.transition3[i] is not None:\n",
        "                if i < self.stage3_cfg['NUM_BRANCHES']:\n",
        "                    x_list.append(self.transition3[i](y_list[i]))\n",
        "                else:\n",
        "                    x_list.append(self.transition3[i](y_list[-1]))\n",
        "            else:\n",
        "                x_list.append(y_list[i])\n",
        "        x = self.stage4(x_list)\n",
        "        #print(x.shape)\n",
        "        # Upsampling\n",
        "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
        "        #x0_h, x0_w = x0_h* 4, x0_w * 2 \n",
        "        #print(x0_h, x0_w)\n",
        "        x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')\n",
        "        x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')\n",
        "        x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')\n",
        "\n",
        "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
        "\n",
        "        x = self.last_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def init_weights(self, pretrained='',):\n",
        "        logger.info('=> init weights from normal distribution')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.001)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        if pretrained:\n",
        "            pretrained_dict = torch.load(pretrained)\n",
        "            logger.info('=> loading pretrained model {}'.format(pretrained))\n",
        "            model_dict = self.state_dict()\n",
        "            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
        "                               if k in model_dict.keys()}\n",
        "            for k, _ in pretrained_dict.items():\n",
        "                logger.info(\n",
        "                    '=> loading {} pretrained model {}'.format(k, pretrained))\n",
        "            model_dict.update(pretrained_dict)\n",
        "            self.load_state_dict(model_dict)\n",
        "\n",
        "def get_seg_model(cfg, **kwargs):\n",
        "    model = HighResolutionNet(cfg, **kwargs)\n",
        "    model.init_weights(cfg['MODEL']['PRETRAINED'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1gFDhm7C-aG"
      },
      "source": [
        "#Create config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9asjZpqhImJ"
      },
      "source": [
        "config = {}\n",
        "\n",
        "config['OUTPUT'] = {\n",
        "    'DIR': './output'\n",
        "}\n",
        "config['WORKERS'] = 4\n",
        "\n",
        "config['DATASET'] = {\n",
        "  'DATASET': 'cityscapes',\n",
        "  'ROOT': 'data/',\n",
        "  'TEST_SET': 'list/cityscapes/val.lst',\n",
        "  'TRAIN_SET': 'list/cityscapes/train.lst',\n",
        "  'NUM_CLASSES': 19,\n",
        "}\n",
        "\n",
        "#config['PRETRAINED'] = None 'PRETRAINED': None,\n",
        "\n",
        "config['MODEL'] = {\n",
        "    'PRETRAINED': None,\n",
        "    'EXTRA':\n",
        "      {'FINAL_CONV_KERNEL': 1,\n",
        "      'STAGE1': {'BLOCK': 'BOTTLENECK', \n",
        "                  'FUSE_METHOD': 'SUM',\n",
        "                  'NUM_BLOCKS': [1],\n",
        "                  'NUM_CHANNELS': [32],\n",
        "                  'NUM_MODULES': 1,\n",
        "                  'NUM_RANCHES': 1\n",
        "                },\n",
        "      'STAGE2': {'BLOCK': 'BASIC',\n",
        "                  'FUSE_METHOD': 'SUM',\n",
        "                  'NUM_BLOCKS': [2, 2],\n",
        "                  'NUM_BRANCHES': 2,\n",
        "                  'NUM_CHANNELS': [16, 32],\n",
        "                  'NUM_MODULES': 1\n",
        "                },\n",
        "      'STAGE3':{'BLOCK': 'BASIC',\n",
        "                'FUSE_METHOD': 'SUM',\n",
        "                'NUM_BLOCKS': [2, 2, 2],\n",
        "                'NUM_BRANCHES': 3,\n",
        "                'NUM_CHANNELS': [16, 32, 64],\n",
        "                'NUM_MODULES': 1\n",
        "                },\n",
        "      'STAGE4': {'BLOCK': 'BASIC',\n",
        "                'FUSE_METHOD': 'SUM',\n",
        "                'NUM_BLOCKS': [2, 2, 2, 2],\n",
        "                'NUM_BRANCHES': 4,\n",
        "                'NUM_CHANNELS': [16, 32, 64, 128],\n",
        "                'NUM_MODULES': 1\n",
        "                }\n",
        "      }\n",
        "}\n",
        "config['LOSS'] = {\n",
        "  'USE_OHEM': False,\n",
        "  'OHEMTHRES': 0.9,\n",
        "  'OHEMKEEP': 131072,\n",
        "}\n",
        "\n",
        "config['TRAIN']={\n",
        "  'IMAGE_SIZE':[1024, 512],\n",
        "  'BASE_SIZE': 2048,\n",
        "  'BATCH_SIZE_PER_GPU': 3,\n",
        "  'SHUFFLE': True,\n",
        "  'BEGIN_EPOCH': 0,\n",
        "  'END_EPOCH': 2, #484\n",
        "  'RESUME': True,\n",
        "  'OPTIMIZER': 'sgd',\n",
        "  'LR': 0.01,\n",
        "  'WD': 0.0005,\n",
        "  'MOMENTUM': 0.9,\n",
        "  'NESTEROV': False,\n",
        "  'FLIP': True,\n",
        "  'MULTI_SCALE': True,\n",
        "  'DOWNSAMPLERATE': 1,\n",
        "  'IGNORE_LABEL': 255,\n",
        "  'SCALE_FACTOR': 16,\n",
        "}\n",
        "config['TEST'] = {\n",
        "  'IMAGE_SIZE':[2048, 1024],\n",
        "  'BASE_SIZE': 2048,\n",
        "  'BATCH_SIZE_PER_GPU': 4,\n",
        "  'FLIP_TEST': False,\n",
        "  'MULTI_SCALE': False,\n",
        "  'NUM_SAMPLES': 0,\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1A7ygd7hWy7"
      },
      "source": [
        "# model = get_seg_model(config)\n",
        "# dump_input = torch.rand((1, 3, 256, 1600))\n",
        "# output = model(dump_input)\n",
        "# print(output.shape)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o60eXf0C6m75"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL5O5J2_CDGF"
      },
      "source": [
        "#Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYCJnr61CKAP"
      },
      "source": [
        "class BaseDataset(data.Dataset):\n",
        "    def __init__(self, \n",
        "                 ignore_label=-1, \n",
        "                 base_size=2048, \n",
        "                 crop_size=(512, 1024), \n",
        "                 downsample_rate=1,\n",
        "                 scale_factor=16,\n",
        "                 mean=[0.485, 0.456, 0.406], \n",
        "                 std=[0.229, 0.224, 0.225]):\n",
        "\n",
        "        self.base_size = base_size\n",
        "        self.crop_size = crop_size\n",
        "        self.ignore_label = ignore_label\n",
        "\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.scale_factor = scale_factor\n",
        "        self.downsample_rate = 1./downsample_rate\n",
        "\n",
        "        self.files = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def input_transform(self, image):\n",
        "        image = image.astype(np.float32)[:, :, ::-1]\n",
        "        image = image / 255.0\n",
        "        image -= self.mean\n",
        "        image /= self.std\n",
        "        return image\n",
        "    \n",
        "    def label_transform(self, label):\n",
        "        return np.array(label).astype('int32')\n",
        "\n",
        "    def pad_image(self, image, h, w, size, padvalue):\n",
        "        pad_image = image.copy()\n",
        "        pad_h = max(size[0] - h, 0)\n",
        "        pad_w = max(size[1] - w, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            pad_image = cv2.copyMakeBorder(image, 0, pad_h, 0, \n",
        "                pad_w, cv2.BORDER_CONSTANT, \n",
        "                value=padvalue)\n",
        "        \n",
        "        return pad_image\n",
        "\n",
        "    def rand_crop(self, image, label):\n",
        "        h, w = image.shape[:-1]\n",
        "        image = self.pad_image(image, h, w, self.crop_size,\n",
        "                                (0.0, 0.0, 0.0))\n",
        "        label = self.pad_image(label, h, w, self.crop_size,\n",
        "                                (self.ignore_label,))\n",
        "        \n",
        "        new_h, new_w = label.shape\n",
        "        x = random.randint(0, new_w - self.crop_size[1])\n",
        "        y = random.randint(0, new_h - self.crop_size[0])\n",
        "        image = image[y:y+self.crop_size[0], x:x+self.crop_size[1]]\n",
        "        label = label[y:y+self.crop_size[0], x:x+self.crop_size[1]]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def multi_scale_aug(self, image, label=None, \n",
        "            rand_scale=1, rand_crop=True):\n",
        "        long_size = np.int(self.base_size * rand_scale + 0.5)\n",
        "        h, w = image.shape[:2]\n",
        "        if h > w:\n",
        "            new_h = long_size\n",
        "            new_w = np.int(w * long_size / h + 0.5)\n",
        "        else:\n",
        "            new_w = long_size\n",
        "            new_h = np.int(h * long_size / w + 0.5)\n",
        "        \n",
        "        image = cv2.resize(image, (new_w, new_h), \n",
        "                           interpolation = cv2.INTER_LINEAR)\n",
        "        if label is not None:\n",
        "            label = cv2.resize(label, (new_w, new_h), \n",
        "                           interpolation = cv2.INTER_NEAREST)\n",
        "        else:\n",
        "            return image\n",
        "        \n",
        "        if rand_crop:\n",
        "            image, label = self.rand_crop(image, label)\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "    def gen_sample(self, image, label, \n",
        "            multi_scale=True, is_flip=True):\n",
        "        if multi_scale:\n",
        "            rand_scale = 0.5 + random.randint(0, self.scale_factor) / 10.0\n",
        "            image, label = self.multi_scale_aug(image, label, \n",
        "                                                    rand_scale=rand_scale)\n",
        "\n",
        "        image = self.input_transform(image)\n",
        "        label = self.label_transform(label)\n",
        "        \n",
        "        image = image.transpose((2, 0, 1))\n",
        "        \n",
        "        if is_flip:\n",
        "            flip = np.random.choice(2) * 2 - 1\n",
        "            image = image[:, :, ::flip]\n",
        "            label = label[:, ::flip]\n",
        "\n",
        "        if self.downsample_rate != 1:\n",
        "            label = cv2.resize(label, \n",
        "                               None, \n",
        "                               fx=self.downsample_rate,\n",
        "                               fy=self.downsample_rate, \n",
        "                               interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def inference(self, model, image, flip=False):\n",
        "        size = image.size()\n",
        "        pred = model(image)\n",
        "        pred = F.upsample(input=pred, \n",
        "                            size=(size[-2], size[-1]), \n",
        "                            mode='bilinear')        \n",
        "        if flip:\n",
        "            flip_img = image.numpy()[:,:,:,::-1]\n",
        "            flip_output = model(torch.from_numpy(flip_img.copy()))\n",
        "            flip_output = F.upsample(input=flip_output, \n",
        "                            size=(size[-2], size[-1]), \n",
        "                            mode='bilinear')\n",
        "            flip_pred = flip_output.cpu().numpy().copy()\n",
        "            flip_pred = torch.from_numpy(flip_pred[:,:,:,::-1].copy()).cuda()\n",
        "            pred += flip_pred\n",
        "            pred = pred * 0.5\n",
        "        return pred.exp()\n",
        "\n",
        "    def multi_scale_inference(self, model, image, scales=[1], flip=False):\n",
        "        batch, _, ori_height, ori_width = image.size()\n",
        "        assert batch == 1, \"only supporting batchsize 1.\"\n",
        "        image = image.numpy()[0].transpose((1,2,0)).copy()\n",
        "        stride_h = np.int(self.crop_size[0] * 2.0 / 3.0)\n",
        "        stride_w = np.int(self.crop_size[1] * 2.0 / 3.0)\n",
        "        final_pred = torch.zeros([1, self.num_classes,\n",
        "                                    ori_height,ori_width]).cuda()\n",
        "        padvalue = -1.0  * np.array(self.mean) / np.array(self.std)\n",
        "        for scale in scales:\n",
        "            new_img = self.multi_scale_aug(image=image,\n",
        "                                           rand_scale=scale,\n",
        "                                           rand_crop=False)\n",
        "            height, width = new_img.shape[:-1]\n",
        "                \n",
        "            if max(height, width) <= np.min(self.crop_size):\n",
        "                new_img = self.pad_image(new_img, height, width, \n",
        "                                    self.crop_size, padvalue)\n",
        "                new_img = new_img.transpose((2, 0, 1))\n",
        "                new_img = np.expand_dims(new_img, axis=0)\n",
        "                new_img = torch.from_numpy(new_img)\n",
        "                preds = self.inference(model, new_img, flip)\n",
        "                preds = preds[:, :, 0:height, 0:width]\n",
        "            else:\n",
        "                if height < self.crop_size[0] or width < self.crop_size[1]:\n",
        "                    new_img = self.pad_image(new_img, height, width, \n",
        "                                        self.crop_size, padvalue)\n",
        "                new_h, new_w = new_img.shape[:-1]\n",
        "                rows = np.int(np.ceil(1.0 * (new_h - \n",
        "                                self.crop_size[0]) / stride_h)) + 1\n",
        "                cols = np.int(np.ceil(1.0 * (new_w - \n",
        "                                self.crop_size[1]) / stride_w)) + 1\n",
        "                preds = torch.zeros([1, self.num_classes,\n",
        "                                           new_h,new_w]).cuda()\n",
        "                count = torch.zeros([1,1, new_h, new_w]).cuda()\n",
        "\n",
        "                for r in range(rows):\n",
        "                    for c in range(cols):\n",
        "                        h0 = r * stride_h\n",
        "                        w0 = c * stride_w\n",
        "                        h1 = min(h0 + self.crop_size[0], new_h)\n",
        "                        w1 = min(w0 + self.crop_size[1], new_w)\n",
        "                        crop_img = new_img[h0:h1, w0:w1, :]\n",
        "                        if h1 == new_h or w1 == new_w:\n",
        "                            crop_img = self.pad_image(crop_img, \n",
        "                                                      h1-h0, \n",
        "                                                      w1-w0, \n",
        "                                                      self.crop_size, \n",
        "                                                      padvalue)\n",
        "                        crop_img = crop_img.transpose((2, 0, 1))\n",
        "                        crop_img = np.expand_dims(crop_img, axis=0)\n",
        "                        crop_img = torch.from_numpy(crop_img)\n",
        "                        pred = self.inference(model, crop_img, flip)\n",
        "                        preds[:,:,h0:h1,w0:w1] += pred[:,:, 0:h1-h0, 0:w1-w0]\n",
        "                        count[:,:,h0:h1,w0:w1] += 1\n",
        "                preds = preds / count\n",
        "                preds = preds[:,:,:height,:width]\n",
        "            preds = F.upsample(preds, (ori_height, ori_width), \n",
        "                                   mode='bilinear')\n",
        "            final_pred += preds\n",
        "        return final_pred\n",
        "\n",
        "\n",
        "class Cityscapes(BaseDataset):\n",
        "    def __init__(self, \n",
        "                 root, \n",
        "                 list_path, \n",
        "                 num_samples=None, \n",
        "                 num_classes=19,\n",
        "                 multi_scale=True, \n",
        "                 flip=True, \n",
        "                 ignore_label=-1, \n",
        "                 base_size=2048, \n",
        "                 crop_size=(512, 1024), \n",
        "                 downsample_rate=1,\n",
        "                 scale_factor=16,\n",
        "                 mean=[0.485, 0.456, 0.406], \n",
        "                 std=[0.229, 0.224, 0.225]):\n",
        "\n",
        "        super(Cityscapes, self).__init__(ignore_label, base_size,\n",
        "                crop_size, downsample_rate, scale_factor, mean, std,)\n",
        "\n",
        "        self.root = root\n",
        "        self.list_path = list_path\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.multi_scale = multi_scale\n",
        "        self.flip = flip\n",
        "        \n",
        "        self.img_list = [line.strip().split() for line in open(root+list_path)]\n",
        "\n",
        "        self.files = self.read_files()\n",
        "        if num_samples:\n",
        "            self.files = self.files[:num_samples]\n",
        "\n",
        "        self.label_mapping = {-1: ignore_label, 0: ignore_label, \n",
        "                              1: ignore_label, 2: ignore_label, \n",
        "                              3: ignore_label, 4: ignore_label, \n",
        "                              5: ignore_label, 6: ignore_label, \n",
        "                              7: 0, 8: 1, 9: ignore_label, \n",
        "                              10: ignore_label, 11: 2, 12: 3, \n",
        "                              13: 4, 14: ignore_label, 15: ignore_label, \n",
        "                              16: ignore_label, 17: 5, 18: ignore_label, \n",
        "                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11,\n",
        "                              25: 12, 26: 13, 27: 14, 28: 15, \n",
        "                              29: ignore_label, 30: ignore_label, \n",
        "                              31: 16, 32: 17, 33: 18}\n",
        "        self.class_weights = torch.FloatTensor([0.8373, 0.918, 0.866, 1.0345, \n",
        "                                        1.0166, 0.9969, 0.9754, 1.0489,\n",
        "                                        0.8786, 1.0023, 0.9539, 0.9843, \n",
        "                                        1.1116, 0.9037, 1.0865, 1.0955, \n",
        "                                        1.0865, 1.1529, 1.0507]).cuda()\n",
        "    \n",
        "    def read_files(self):\n",
        "        files = []\n",
        "        if 'test' in self.list_path:\n",
        "            for item in self.img_list:\n",
        "                image_path = item\n",
        "                name = os.path.splitext(os.path.basename(image_path[0]))[0]\n",
        "                files.append({\n",
        "                    \"img\": image_path[0],\n",
        "                    \"name\": name,\n",
        "                })\n",
        "        else:\n",
        "            for item in self.img_list:\n",
        "                image_path, label_path = item\n",
        "                name = os.path.splitext(os.path.basename(label_path))[0]\n",
        "                files.append({\n",
        "                    \"img\": image_path,\n",
        "                    \"label\": label_path,\n",
        "                    \"name\": name,\n",
        "                    \"weight\": 1\n",
        "                })\n",
        "        return files\n",
        "        \n",
        "    def convert_label(self, label, inverse=False):\n",
        "        temp = label.copy()\n",
        "        if inverse:\n",
        "            for v, k in self.label_mapping.items():\n",
        "                label[temp == k] = v\n",
        "        else:\n",
        "            for k, v in self.label_mapping.items():\n",
        "                label[temp == k] = v\n",
        "        return label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.files[index]\n",
        "        name = item[\"name\"]\n",
        "        image = cv2.imread(os.path.join(self.root,'cityscapes',item[\"img\"]),\n",
        "                           cv2.IMREAD_COLOR)\n",
        "        size = image.shape\n",
        "\n",
        "        if 'test' in self.list_path:\n",
        "            image = self.input_transform(image)\n",
        "            image = image.transpose((2, 0, 1))\n",
        "\n",
        "            return image.copy(), np.array(size), name\n",
        "\n",
        "        label = cv2.imread(os.path.join(self.root,'cityscapes',item[\"label\"]),\n",
        "                           cv2.IMREAD_GRAYSCALE)\n",
        "        label = self.convert_label(label)\n",
        "\n",
        "        image, label = self.gen_sample(image, label, \n",
        "                                self.multi_scale, self.flip)\n",
        "\n",
        "        return image.copy(), label.copy(), np.array(size), name\n",
        "\n",
        "    def multi_scale_inference(self, model, image, scales=[1], flip=False):\n",
        "        batch, _, ori_height, ori_width = image.size()\n",
        "        assert batch == 1, \"only supporting batchsize 1.\"\n",
        "        image = image.numpy()[0].transpose((1,2,0)).copy()\n",
        "        stride_h = np.int(self.crop_size[0] * 1.0)\n",
        "        stride_w = np.int(self.crop_size[1] * 1.0)\n",
        "        final_pred = torch.zeros([1, self.num_classes,\n",
        "                                    ori_height,ori_width]).cuda()\n",
        "        for scale in scales:\n",
        "            new_img = self.multi_scale_aug(image=image,\n",
        "                                           rand_scale=scale,\n",
        "                                           rand_crop=False)\n",
        "            height, width = new_img.shape[:-1]\n",
        "                \n",
        "            if scale <= 1.0:\n",
        "                new_img = new_img.transpose((2, 0, 1))\n",
        "                new_img = np.expand_dims(new_img, axis=0)\n",
        "                new_img = torch.from_numpy(new_img)\n",
        "                preds = self.inference(model, new_img, flip)\n",
        "                preds = preds[:, :, 0:height, 0:width]\n",
        "            else:\n",
        "                new_h, new_w = new_img.shape[:-1]\n",
        "                rows = np.int(np.ceil(1.0 * (new_h - \n",
        "                                self.crop_size[0]) / stride_h)) + 1\n",
        "                cols = np.int(np.ceil(1.0 * (new_w - \n",
        "                                self.crop_size[1]) / stride_w)) + 1\n",
        "                preds = torch.zeros([1, self.num_classes,\n",
        "                                           new_h,new_w]).cuda()\n",
        "                count = torch.zeros([1,1, new_h, new_w]).cuda()\n",
        "\n",
        "                for r in range(rows):\n",
        "                    for c in range(cols):\n",
        "                        h0 = r * stride_h\n",
        "                        w0 = c * stride_w\n",
        "                        h1 = min(h0 + self.crop_size[0], new_h)\n",
        "                        w1 = min(w0 + self.crop_size[1], new_w)\n",
        "                        h0 = max(int(h1 - self.crop_size[0]), 0)\n",
        "                        w0 = max(int(w1 - self.crop_size[1]), 0)\n",
        "                        crop_img = new_img[h0:h1, w0:w1, :]\n",
        "                        crop_img = crop_img.transpose((2, 0, 1))\n",
        "                        crop_img = np.expand_dims(crop_img, axis=0)\n",
        "                        crop_img = torch.from_numpy(crop_img)\n",
        "                        pred = self.inference(model, crop_img, flip)\n",
        "                        preds[:,:,h0:h1,w0:w1] += pred[:,:, 0:h1-h0, 0:w1-w0]\n",
        "                        count[:,:,h0:h1,w0:w1] += 1\n",
        "                preds = preds / count\n",
        "                preds = preds[:,:,:height,:width]\n",
        "            preds = F.upsample(preds, (ori_height, ori_width), \n",
        "                                   mode='bilinear')\n",
        "            final_pred += preds\n",
        "        return final_pred\n",
        "\n",
        "    def get_palette(self, n):\n",
        "        palette = [0] * (n * 3)\n",
        "        for j in range(0, n):\n",
        "            lab = j\n",
        "            palette[j * 3 + 0] = 0\n",
        "            palette[j * 3 + 1] = 0\n",
        "            palette[j * 3 + 2] = 0\n",
        "            i = 0\n",
        "            while lab:\n",
        "                palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n",
        "                palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n",
        "                palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n",
        "                i += 1\n",
        "                lab >>= 3\n",
        "        return palette\n",
        "\n",
        "    def save_pred(self, preds, sv_path, name):\n",
        "        palette = self.get_palette(256)\n",
        "        preds = np.asarray(np.argmax(preds, axis=1), dtype=np.uint8)\n",
        "        for i in range(preds.shape[0]):\n",
        "            pred = self.convert_label(preds[i], inverse=True)\n",
        "            save_img = Image.fromarray(pred)\n",
        "            save_img.putpalette(palette)\n",
        "            save_img.save(os.path.join(sv_path, name[i]+'.png'))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJHNOZZHDE5Q"
      },
      "source": [
        "#Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCJ_f8s5vCUv"
      },
      "source": [
        "def train(config, epoch, num_epoch, epoch_iters, base_lr, \n",
        "        num_iters, trainloader, optimizer, model, writer_dict):\n",
        "    # Training\n",
        "    model.train()\n",
        "    batch_time = AverageMeter()\n",
        "    ave_loss = AverageMeter()\n",
        "    tic = time.time()\n",
        "    cur_iters = epoch*epoch_iters\n",
        "    writer = writer_dict['writer']\n",
        "    global_steps = writer_dict['train_global_steps']\n",
        "    for i_iter, batch in enumerate(trainloader, 0):\n",
        "        images, labels, _, _ = batch\n",
        "        labels = labels.long()\n",
        "\n",
        "        print(sys.getsizeof(images))\n",
        "        print(sys.getsizeof(labels))\n",
        "        if torch.cuda.is_available():\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "        losses, _ = model(images, labels)\n",
        "        loss = losses.mean()\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - tic)\n",
        "        tic = time.time()\n",
        "\n",
        "        # update average loss\n",
        "        ave_loss.update(loss.item())\n",
        "\n",
        "        lr = adjust_learning_rate(optimizer,\n",
        "                                  base_lr,\n",
        "                                  num_iters,\n",
        "                                  i_iter+cur_iters)\n",
        "\n",
        "        if i_iter % config.PRINT_FREQ == 0:\n",
        "            msg = 'Epoch: [{}/{}] Iter:[{}/{}], Time: {:.2f}, ' \\\n",
        "                  'lr: {:.6f}, Loss: {:.6f}' .format(\n",
        "                      epoch, num_epoch, i_iter, epoch_iters, \n",
        "                      batch_time.average(), lr, ave_loss.average())\n",
        "            logging.info(msg)\n",
        "\n",
        "    writer.add_scalar('train_loss', ave_loss.average(), global_steps)\n",
        "    writer_dict['train_global_steps'] = global_steps + 1\n",
        "\n",
        "def validate(config, testloader, model, writer_dict):\n",
        "    model.eval()\n",
        "    ave_loss = AverageMeter()\n",
        "    confusion_matrix = np.zeros(\n",
        "        (config.DATASET.NUM_CLASSES, config.DATASET.NUM_CLASSES))\n",
        "    with torch.no_grad():\n",
        "        for _, batch in enumerate(testloader):\n",
        "            image, label, _, _ = batch\n",
        "            size = label.size().to(device)\n",
        "            label = label.long().to(device)\n",
        "            losses, pred = model(image, label)\n",
        "            pred = F.upsample(input=pred, size=(\n",
        "                        size[-2], size[-1]), mode='bilinear')\n",
        "            loss = losses.mean()\n",
        "            ave_loss.update(loss.item())\n",
        "\n",
        "            confusion_matrix += get_confusion_matrix(\n",
        "                label,\n",
        "                pred,\n",
        "                size,\n",
        "                config.DATASET.NUM_CLASSES,\n",
        "                config.TRAIN.IGNORE_LABEL)\n",
        "\n",
        "    pos = confusion_matrix.sum(1)\n",
        "    res = confusion_matrix.sum(0)\n",
        "    tp = np.diag(confusion_matrix)\n",
        "    IoU_array = (tp / np.maximum(1.0, pos + res - tp))\n",
        "    mean_IoU = IoU_array.mean()\n",
        "\n",
        "    writer = writer_dict['writer']\n",
        "    global_steps = writer_dict['valid_global_steps']\n",
        "    writer.add_scalar('valid_loss', ave_loss.average(), global_steps)\n",
        "    writer.add_scalar('valid_mIoU', mean_IoU, global_steps)\n",
        "    writer_dict['valid_global_steps'] = global_steps + 1\n",
        "    return ave_loss.average(), mean_IoU, IoU_array\n",
        "\n",
        "def testval(config, test_dataset, testloader, model, \n",
        "        sv_dir='', sv_pred=False):\n",
        "    model.eval()\n",
        "    confusion_matrix = np.zeros(\n",
        "        (config.DATASET.NUM_CLASSES, config.DATASET.NUM_CLASSES))\n",
        "    with torch.no_grad():\n",
        "        for index, batch in enumerate(tqdm(testloader)):\n",
        "            image, label, _, name = batch\n",
        "            size = label.size()\n",
        "            pred = test_dataset.multi_scale_inference(\n",
        "                        model, \n",
        "                        image, \n",
        "                        scales=config.TEST.SCALE_LIST, \n",
        "                        flip=config.TEST.FLIP_TEST)\n",
        "            \n",
        "            if pred.size()[-2] != size[-2] or pred.size()[-1] != size[-1]:\n",
        "                pred = F.upsample(pred, (size[-2], size[-1]), \n",
        "                                   mode='bilinear')\n",
        "\n",
        "            confusion_matrix += get_confusion_matrix(\n",
        "                label,\n",
        "                pred,\n",
        "                size,\n",
        "                config.DATASET.NUM_CLASSES,\n",
        "                config.TRAIN.IGNORE_LABEL)\n",
        "\n",
        "            if sv_pred:\n",
        "                sv_path = os.path.join(sv_dir,'test_results')\n",
        "                if not os.path.exists(sv_path):\n",
        "                    os.mkdir(sv_path)\n",
        "                test_dataset.save_pred(pred, sv_path, name)\n",
        "            \n",
        "            if index % 100 == 0:\n",
        "                logging.info('processing: %d images' % index)\n",
        "                pos = confusion_matrix.sum(1)\n",
        "                res = confusion_matrix.sum(0)\n",
        "                tp = np.diag(confusion_matrix)\n",
        "                IoU_array = (tp / np.maximum(1.0, pos + res - tp))\n",
        "                mean_IoU = IoU_array.mean()\n",
        "                logging.info('mIoU: %.4f' % (mean_IoU))\n",
        "\n",
        "    pos = confusion_matrix.sum(1)\n",
        "    res = confusion_matrix.sum(0)\n",
        "    tp = np.diag(confusion_matrix)\n",
        "    pixel_acc = tp.sum()/pos.sum()\n",
        "    mean_acc = (tp/np.maximum(1.0, pos)).mean()\n",
        "    IoU_array = (tp / np.maximum(1.0, pos + res - tp))\n",
        "    mean_IoU = IoU_array.mean()\n",
        "\n",
        "    return mean_IoU, IoU_array, pixel_acc, mean_acc\n",
        "\n",
        "def test(config, test_dataset, testloader, model, \n",
        "        sv_dir='', sv_pred=True):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, batch in enumerate(tqdm(testloader)):\n",
        "            image, size, name = batch\n",
        "            size = size[0]\n",
        "            pred = test_dataset.multi_scale_inference(\n",
        "                        model, \n",
        "                        image, \n",
        "                        scales=config.TEST.SCALE_LIST, \n",
        "                        flip=config.TEST.FLIP_TEST)\n",
        "            \n",
        "            if pred.size()[-2] != size[0] or pred.size()[-1] != size[1]:\n",
        "                pred = F.upsample(pred, (size[-2], size[-1]), \n",
        "                                   mode='bilinear')\n",
        "\n",
        "            if sv_pred:\n",
        "                sv_path = os.path.join(sv_dir,'test_results')\n",
        "                if not os.path.exists(sv_path):\n",
        "                    os.mkdir(sv_path)\n",
        "                test_dataset.save_pred(pred, sv_path, name)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "vDRb0aXAJEZE",
        "outputId": "d8eab528-31bc-435a-e1b4-464235a3861e"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# build model\n",
        "model = get_seg_model(config)\n",
        "dump_input = torch.rand((1, 3, config['TRAIN']['IMAGE_SIZE'][1], config['TRAIN']['IMAGE_SIZE'][0])).to(device)\n",
        "\n",
        "\n",
        "# copy model file\n",
        "#this_dir = '/HRNet-Semantic-Segmentation'\n",
        "final_output_dir = config['OUTPUT']['DIR']\n",
        "models_dst_dir = os.path.join(final_output_dir, 'models')\n",
        "if os.path.exists(models_dst_dir):\n",
        "    shutil.rmtree(models_dst_dir)\n",
        "shutil.copytree('./lib/models', models_dst_dir)\n",
        "gpus = 1\n",
        "\n",
        "# prepare data\n",
        "crop_size = (config['TRAIN']['IMAGE_SIZE'][1], config['TRAIN']['IMAGE_SIZE'][0])\n",
        "train_dataset = Cityscapes(\n",
        "                    root=config['DATASET']['ROOT'],\n",
        "                    list_path=config['DATASET']['TRAIN_SET'],\n",
        "                    num_samples=None,\n",
        "                    num_classes=config['DATASET']['NUM_CLASSES'],\n",
        "                    multi_scale=config['TRAIN']['MULTI_SCALE'],\n",
        "                    flip=config['TRAIN']['FLIP'],\n",
        "                    ignore_label=config['TRAIN']['IGNORE_LABEL'],\n",
        "                    base_size=config['TRAIN']['BASE_SIZE'],\n",
        "                    crop_size=crop_size,\n",
        "                    downsample_rate=config['TRAIN']['DOWNSAMPLERATE'],\n",
        "                    scale_factor=config['TRAIN']['SCALE_FACTOR'])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "                  train_dataset,\n",
        "                  batch_size=config['TRAIN']['BATCH_SIZE_PER_GPU']*gpus,\n",
        "                  shuffle=config['TRAIN']['SHUFFLE'],\n",
        "                  num_workers=config['WORKERS'],\n",
        "                  pin_memory=True,\n",
        "                  drop_last=True)\n",
        "\n",
        "test_size = (config['TEST']['IMAGE_SIZE'][1], config['TEST']['IMAGE_SIZE'][0])\n",
        "test_dataset = Cityscapes(\n",
        "                    root=config['DATASET']['ROOT'],\n",
        "                    list_path=config['DATASET']['TEST_SET'],\n",
        "                    num_samples=config['TEST']['NUM_SAMPLES'],\n",
        "                    num_classes=config['DATASET']['NUM_CLASSES'],\n",
        "                    multi_scale=False,\n",
        "                    flip=False,\n",
        "                    ignore_label=config['TRAIN']['IGNORE_LABEL'],\n",
        "                    base_size=config['TEST']['BASE_SIZE'],\n",
        "                    crop_size=test_size,\n",
        "                    downsample_rate=1)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['TEST']['BATCH_SIZE_PER_GPU']*gpus,\n",
        "        shuffle=False,\n",
        "        num_workers=config['WORKERS'],\n",
        "        pin_memory=True)\n",
        "\n",
        "\n",
        "# criterion\n",
        "if config['LOSS']['USE_OHEM']:\n",
        "    criterion = OhemCrossEntropy(ignore_label=config['TRAIN']['IGNORE_LABEL'],\n",
        "                                  thres=config['LOSS']['OHEMTHRES'],\n",
        "                                  min_kept=config['LOSS']['OHEMKEEP'],\n",
        "                                  weight=train_dataset.class_weights)\n",
        "else:\n",
        "    criterion = CrossEntropy(ignore_label=config['TRAIN']['IGNORE_LABEL'],\n",
        "                              weight=train_dataset.class_weights)\n",
        "\n",
        "model = FullModel(model, criterion)\n",
        "print(sys.getsizeof(model))\n",
        "if torch.cuda.is_available():\n",
        "  model = model.to(device)\n",
        "\n",
        "# optimizer\n",
        "if config['TRAIN']['OPTIMIZER'] == 'sgd':\n",
        "    optimizer = torch.optim.SGD([{'params':\n",
        "                              filter(lambda p: p.requires_grad,\n",
        "                                      model.parameters()),\n",
        "                              'lr': config['TRAIN']['LR']}],\n",
        "                            lr=config['TRAIN']['LR'],\n",
        "                            momentum=config['TRAIN']['MOMENTUM'],\n",
        "                            weight_decay=config['TRAIN']['WD'],\n",
        "                            nesterov=config['TRAIN']['NESTEROV'],\n",
        "                            )\n",
        "else:\n",
        "    raise ValueError('Only Support SGD optimizer')\n",
        "\n",
        "\n",
        "epoch_iters = np.int(train_dataset.__len__() / \n",
        "                        config['TRAIN']['BATCH_SIZE_PER_GPU'] / gpus)\n",
        "best_mIoU = 0\n",
        "last_epoch = 0\n",
        "if config['TRAIN']['RESUME']:\n",
        "    model_state_file = os.path.join(final_output_dir, 'checkpoint.pth.tar')\n",
        "    if os.path.isfile(model_state_file):#消してもいい\n",
        "        checkpoint = torch.load(model_state_file)\n",
        "        best_mIoU = checkpoint['best_mIoU']\n",
        "        last_epoch = checkpoint['epoch']\n",
        "        model.module.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        #logger.info(\"=> loaded checkpoint (epoch {})\".format(checkpoint['epoch']))\n",
        "\n",
        "tb_log_dir = 'log/'\n",
        "writer_dict = {\n",
        "        'writer': SummaryWriter(tb_log_dir),\n",
        "        'train_global_steps': 0,\n",
        "        'valid_global_steps': 0,\n",
        "}\n",
        "start = timeit.default_timer()\n",
        "end_epoch = config['TRAIN']['END_EPOCH']\n",
        "num_iters = config['TRAIN']['END_EPOCH'] * epoch_iters\n",
        "extra_iters = 0 * epoch_iters\n",
        "\n",
        "for epoch in range(last_epoch, end_epoch):\n",
        "    if epoch < config['TRAIN']['END_EPOCH']:\n",
        "        train(config, epoch, config['TRAIN']['END_EPOCH'], \n",
        "              epoch_iters, config['TRAIN']['LR'], num_iters,\n",
        "              trainloader, optimizer, model, writer_dict)\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': epoch+1,\n",
        "        'best_mIoU': best_mIoU,\n",
        "        'state_dict': model.module.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }, os.path.join(final_output_dir,'checkpoint.pth.tar'))\n",
        "    valid_loss, mean_IoU, IoU_array = validate(\n",
        "                    config, testloader, model, writer_dict)\n",
        "    if mean_IoU > best_mIoU:\n",
        "        best_mIoU = mean_IoU\n",
        "        torch.save(model.module.state_dict(),\n",
        "                    os.path.join(final_output_dir, 'best.pth'))\n",
        "    msg = 'Loss: {:.3f}, MeanIU: {: 4.4f}, Best_mIoU: {: 4.4f}'.format(\n",
        "                valid_loss, mean_IoU, best_mIoU)\n",
        "    print(msg)\n",
        "    print(IoU_array)\n",
        "    #logging.info(msg)\n",
        "    #logging.info(IoU_array)\n",
        "\n",
        "torch.save(model.module.state_dict(), os.path.join(final_output_dir, 'final_state.pth'))\n",
        "\n",
        "writer_dict['writer'].close()\n",
        "end = timeit.default_timer()\n",
        "print('Hours: %d' % np.int((end-start)/3600))\n",
        "print('Done')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "56\n",
            "72\n",
            "72\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-273610eb407c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         train(config, epoch, config['TRAIN']['END_EPOCH'], \n\u001b[1;32m    120\u001b[0m               \u001b[0mepoch_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TRAIN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m               trainloader, optimizer, model, writer_dict)\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     torch.save({\n",
            "\u001b[0;32m<ipython-input-13-301b8db2052f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, epoch, num_epoch, epoch_iters, base_lr, num_iters, trainloader, optimizer, model, writer_dict)\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b2fa48e687df>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-377fe3afadb8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mx_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Upsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-fe558bfc60e0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                       \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheight_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                       mode='bilinear')\n\u001b[0m\u001b[1;32m    132\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 14.73 GiB total capacity; 13.71 GiB already allocated; 71.88 MiB free; 13.72 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOz4FVSZPvJa"
      },
      "source": [
        "!nvidia-smi -q -d MEMORY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UOTa_gkQLno",
        "outputId": "911640e6-ca1a-4250-dd7b-72c36030816c"
      },
      "source": [
        "!nvidia-smi\n",
        "!lsof /dev/nvidia*"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec 22 07:23:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    29W /  70W |  15013MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "COMMAND  PID USER   FD   TYPE  DEVICE SIZE/OFF  NODE NAME\n",
            "python3 2383 root  mem    CHR 195,255          18249 /dev/nvidiactl\n",
            "python3 2383 root  mem    CHR   195,0          18248 /dev/nvidia0\n",
            "python3 2383 root  mem    CHR   246,0          18246 /dev/nvidia-uvm\n",
            "python3 2383 root   61u   CHR 195,255      0t0 18249 /dev/nvidiactl\n",
            "python3 2383 root   62u   CHR   246,0      0t0 18246 /dev/nvidia-uvm\n",
            "python3 2383 root   63u   CHR   195,0      0t0 18248 /dev/nvidia0\n",
            "python3 2383 root   64u   CHR   195,0      0t0 18248 /dev/nvidia0\n",
            "python3 2383 root   65u   CHR   195,0      0t0 18248 /dev/nvidia0\n",
            "python3 2383 root   70u   CHR   195,0      0t0 18248 /dev/nvidia0\n",
            "python3 2383 root   71u   CHR   195,0      0t0 18248 /dev/nvidia0\n",
            "python3 2383 root   72u   CHR   195,0      0t0 18248 /dev/nvidia0\n",
            "python3 2383 root   73u   CHR   195,0      0t0 18248 /dev/nvidia0\n",
            "python3 2383 root   76u   CHR   195,0      0t0 18248 /dev/nvidia0\n",
            "python3 2383 root   77u   CHR   195,0      0t0 18248 /dev/nvidia0\n",
            "python3 2383 root   78u   CHR   195,0      0t0 18248 /dev/nvidia0\n",
            "python3 2383 root   79u   CHR   195,0      0t0 18248 /dev/nvidia0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtqTjJrCRGFm"
      },
      "source": [
        "!kill -9 $(lsof -t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJE0TPP3jU6J"
      },
      "source": [
        "# num_images = 1000                                   #生成する画像数\n",
        "# length = 64                                          #画像のサイズ\n",
        "# imgs = np.zeros([num_images, 3, length, length])     #ゼロ行列を生成,入力画像\n",
        "# imgs_ano = np.zeros([num_images, 4, length, length]) #出力画像\n",
        "\n",
        "# for i in range(num_images):\n",
        "#     centers = []\n",
        "#     img = np.zeros([length, length])\n",
        "#     img_ano = np.zeros([64, 64])\n",
        "#     for j in range(6):                       #四角形を最大6つ生成\n",
        "#         img, img_ano, centers = rectangle(img, img_ano, centers, 12) \n",
        "#     imgs[i, 0, :, :] = img\n",
        "#     imgs_ano[i, 0, :, :] = img_ano\n",
        "\n",
        "# imgs = torch.tensor(imgs, dtype = torch.float32)                 #ndarray - torch.tensor\n",
        "# imgs_ano = torch.tensor(imgs_ano, dtype = torch.float32)           #ndarray - torch.tensor\n",
        "# data_set = TensorDataset(imgs, imgs_ano)\n",
        "# data_loader = DataLoader(data_set, batch_size = 100, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrMdFZgAjYAP"
      },
      "source": [
        "#before\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "# net = get_seg_model(config).to(device)                \n",
        "# loss_fn = nn.MSELoss()                                #損失関数の定義\n",
        "# optimizer = optim.Adam(net.parameters(), lr = 0.01)\n",
        "\n",
        "# losses = []                                     #epoch毎のlossを記録\n",
        "# epoch_time = 30\n",
        "\n",
        "# for epoch in range(epoch_time):\n",
        "#     running_loss = 0.0                          #epoch毎のlossの計算\n",
        "#     net.train()\n",
        "#     for i, (XX, yy) in enumerate(data_loader):\n",
        "#         optimizer.zero_grad()  \n",
        "#         XX = XX.to(device) \n",
        "#         yy = yy.to(device)    \n",
        "#         y_pred = net(XX)\n",
        "#         loss = loss_fn(y_pred, yy)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "#     print(\"epoch:\",epoch, \" loss:\", running_loss/(i + 1))\n",
        "#     losses.append(running_loss/(i + 1))\n",
        "\n",
        "# #lossの可視化\n",
        "# plt.plot(losses)\n",
        "# plt.ylabel(\"loss\")\n",
        "# plt.xlabel(\"epoch time\")\n",
        "# plt.savefig(\"loss_auto\")\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUujZ9bZDLEK"
      },
      "source": [
        "#Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0aTS3b1pMje"
      },
      "source": [
        "# net.eval()            #評価モード\n",
        "# #今まで学習していない画像を1つ生成\n",
        "# num_images = 1\n",
        "# img_test = np.zeros([num_images, 3, length, length])\n",
        "# imgs_test_ano = np.zeros([num_images, 4, length, length])\n",
        "# for i in range(num_images):\n",
        "#     centers = []\n",
        "#     img = np.zeros([length, length])\n",
        "#     img_ano = np.zeros([length, length])\n",
        "#     for j in range(6):\n",
        "#         img, img_ano, centers = rectangle(img, img_ano, centers, 7)\n",
        "#     img_test[i, 0, :, :] = img\n",
        "\n",
        "# img_test = img_test.reshape([1, 3, 64, 64])\n",
        "# img_test = torch.tensor(img_test, dtype = torch.float32).to(device)\n",
        "# img_test = net(img_test).to(device)             #生成した画像を学習済のネットワークへ\n",
        "# img_test = img_test.cpu().detach().numpy() #torch.tensor - ndarray\n",
        "# img_test = img_test[0, 0, :, :]\n",
        "\n",
        "# plt.imshow(img, cmap = \"gray\")       #inputデータの可視化\n",
        "# plt.savefig(\"input_auto\")\n",
        "# plt.show()\n",
        "# plt.imshow(img_test, cmap = \"gray\")  #outputデータの可視化\n",
        "# plt.savefig(\"output_auto\")\n",
        "# plt.show()\n",
        "# plt.imshow(img_ano, cmap = \"gray\")   #正解データ\n",
        "# plt.savefig(\"correct_auto\")\n",
        "# plt.plot()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}